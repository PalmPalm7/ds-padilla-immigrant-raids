{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "written by Can Erozer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finding each of the average value, I will use 30 samples. This sample size is not huge since I don't want to spend a lot of money just on this calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the mean cost of scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.schema import StrOutputParser\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These urls are picked randomly from the Raids data and search_results - Abnormal Arrest Days, All Days All Counties.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[\"https://www.cbs58.com/news/34-wisconsin-men-were-arrested-by-immigration-and-customs-enforcement\",\\\n",
    "     \"https://abc7chicago.com/ice-raid-los-angeles-southern-california-four-day-record-244-criminal-immigrants/964411/\",\\\n",
    "     \"https://www.justice.gov/opa/pr/fourteen-alleged-gang-members-and-associates-indicted-charleston-south-carolina-federal\",\\\n",
    "     \"https://www.hjnews.com/education/cache-high-senior-works-stays-positive-despite-dads-deportation/article_96d1bd75-0ac0-5ed5-a885-cad032bee9ea.html\",\\\n",
    "     \"https://www.noozhawk.com/12_arrested_40_in_custody_after_santa_maria_police_led_sweep/\",\\\n",
    "     \"https://www.azcentral.com/story/noticias/2016/06/17/arrestan-39-inmigrantes-durante-redada-en-wisconsin/86054218/\",\\\n",
    "     \"https://www.ice.gov/news/releases/more-100-arrested-los-angeles-area-ice-operation-targeting-convicted-criminal-aliens\",\\\n",
    "     \"https://www.dnainfo.com/chicago/20160811/avondale/this-is-wrong-latino-day-laborer-declares-of-ice-raids-at-job-sites/\",\\\n",
    "     \"https://www.fox8live.com/story/33050506/undocumented-immigrants-arrested-for-operating-new-orleans-sex-brothel/\",\\\n",
    "     \"https://weartv.com/news/local/investigators-man-shared-child-pornography-on-kid-friendly-kik-app\",\\\n",
    "     \"https://www.justice.gov/usao-nh/pr/45-individuals-indicted-participating-fentanyl-trafficking-conspiracy\",\\\n",
    "     \"https://www.nj.com/news/2018/04/ice_arrests_60_in_nj_in_5-day_enforcement_operatio.html\",\\\n",
    "     \"https://www.latimes.com/opinion/readersreact/la-le-0819-wednesday-immigration-deport-20150819-story.html\",\\\n",
    "    \"https://www.safepassageproject.org/2018/02/safe-passage-founder-lenni-benson-comments-on-ice-arrests/\",\\\n",
    "    \"https://link.springer.com/article/10.1007/s41134-016-0018-8\",\\\n",
    "    \"https://www.cbsnews.com/news/one-mans-mission-to-rescue-child-sex-trafficking-victims/\",\\\n",
    "    \"https://www.latimes.com/nation/nationnow/la-na-houston-immigrant-clinic-arrest-20150914-story.html\",\\\n",
    "    \"https://santaclara.courts.ca.gov/divisions/civil-division\",\\\n",
    "    \"https://www.newsweek.com/why-did-immigration-and-customs-enforcement-detain-us-citizen-3-and-half-years-282509\",\\\n",
    "    \"https://www.cbsnews.com/colorado/news/immigration-deportation-michael-hancock/\",\\\n",
    "    \"https://finbaroneill.blogspot.com/2014/12/jail-release-could-depend-on-prediction.html\",\\\n",
    "    \"https://www.kcbd.com/story/14664183/immigration-raid-in-odessa-targeted-mcdougal-construction-site/\",\\\n",
    "    \"https://www.nytimes.com/2016/10/05/us/us-immigration-judges-bias.html\",\\\n",
    "    \"https://hightimes.com/culture/federal-raids-in-colorado-tell-larger-story/\",\\\n",
    "    \"https://www.cbp.gov/border-security/along-us-borders/border-patrol-sectors/rio-grande-valley-sector-texas/mcallen-station\",\\\n",
    "    \"https://www.cnn.com/2017/02/18/politics/kelly-guidance-on-immigration-and-border-security/index.html\",\\\n",
    "    \"https://france-amerique.com/300-years-of-french-culture-in-alabama/\",\\\n",
    "    \"https://www.univision.com/univision-news/opinion/the-story-of-the-la-dad-detained-by-ice-after-dropping-daughter-at-school\",\\\n",
    "    \"https://www.npr.org/sections/thetwo-way/2018/01/31/582307323/feds-formalize-plan-to-pursue-certain-undocumented-immigrants-in-courthouses\",\\\n",
    "    \"https://www.buzzfeednews.com/article/jasonwells/ice-arrests-up-since-trump\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_extract_info=\"\"\"I have scraped the text of an article, but it contains extra information like side titles, navigation menus, and other irrelevant content. I also have the title of the article. Please extract only the relevant parts of the text that match the main topic of the article based on its title. Ignore anything unrelated to the topic. \n",
    "\n",
    "Here is the contex: {context}\n",
    "\n",
    "Here is the title: {title}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Focus only on the parts of the text that relate directly to the title and the main topic of the article. If the title is provided as None, focus only on the parts of the text that relate directly to the main topic of the article.\n",
    "Don't forget to include the important dates in the article. Also, include the publication date of the article if it is present in the text.\n",
    "Remove any information that is clearly unrelated, like website headers, footers, links, or side titles.\n",
    "Provide the cleaned-up and relevant text as the output.\n",
    "\n",
    "\n",
    "\n",
    "Output format:\n",
    "\n",
    "A cleaned and concise version of the article text, free of irrelevant information. Don't make any explanations.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this returns a tuple(str, str). The first element of the tuple is the title of the article\n",
    "#and the second element of the tuple is the all of the text present in the url\n",
    "def get_all_texts_from_url(article_urls):\n",
    "    \n",
    "    all_texts=[]\n",
    "    \n",
    "    for url in article_urls:\n",
    "    \n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Get all visible text from the webpage\n",
    "            article_text = soup.get_text(strip=True)\n",
    "            \n",
    "            # Extract text from h1, h2, and p tags\n",
    "            headings_and_paragraphs = []\n",
    "            for tag in soup.find_all(['h1']):\n",
    "                headings_and_paragraphs.append(tag.get_text(strip=True))\n",
    "                \n",
    "            if len(headings_and_paragraphs)==0:\n",
    "                for tag in soup.find_all(['h2']):\n",
    "                    headings_and_paragraphs.append(tag.get_text(strip=True))\n",
    "                \n",
    "            if len(headings_and_paragraphs)==0:\n",
    "                print(\"couldn't find the title\")\n",
    "                \n",
    "                all_texts +=[(None, article_text)]\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                all_texts +=[(headings_and_paragraphs[0], article_text)]\n",
    "                \n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    return all_texts\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_relevant_text(title_text_pairs, model_name):\n",
    "    \n",
    "    main_text=[]\n",
    "    cost=0\n",
    "    \n",
    "    for title, context in tqdm(title_text_pairs):        \n",
    "    \n",
    "        PROMPT0 = PromptTemplate(template=prompt_extract_info, input_variables=[\"context\", \"title\"])\n",
    "\n",
    "        with get_openai_callback() as cb0:\n",
    "            llm = LLMChain(\n",
    "                llm = ChatOpenAI(openai_api_key=api_key,\n",
    "                         temperature=0.01, model=model_name), prompt=PROMPT0)\n",
    "\n",
    "            response0 = llm.predict(context=context, title=title)\n",
    "            \n",
    "            print(f\"cost for this scraping:{cb0.total_cost}\")\n",
    "\n",
    "            cost +=cb0.total_cost\n",
    "            \n",
    "            print(\"scraped!\")\n",
    "\n",
    "\n",
    "        main_text +=[title + response0]\n",
    "        \n",
    "        \n",
    "    return main_text, cost\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pred_texts(pred_texts, out_file_names):\n",
    "    \n",
    "    directory_out=\"/Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\"\n",
    "    \n",
    "    file_names=[]\n",
    "    \n",
    "    for i in range(len(pred_texts)):\n",
    "        \n",
    "        name = \"pred_\" + out_file_names[i]\n",
    "        file_names +=[out_file_names[i]]\n",
    "     \n",
    "    #for name in out_file_names:\n",
    "        #name = \"pred_\" + name\n",
    "        #file_names +=[name]\n",
    "        \n",
    "    #if len(file_names)!=len(pred_texts):\n",
    "        #print(\"It seems that some articles did not scraped!\")\n",
    "        \n",
    "        \n",
    "    for i in range(len(file_names)):\n",
    "\n",
    "        with open(directory_out+file_names[i], \"w\",encoding='utf-8') as file:\n",
    "\n",
    "            file.write(pred_texts[i])\n",
    "            print(f\"{file_names[i]} written to {directory_out}\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(urls, test_files):\n",
    "    \n",
    "    \n",
    "    results=get_all_texts_from_url(urls)\n",
    "    \n",
    "    cleaned_texts, cost=get_only_relevant_text(results, \"gpt-4o-mini\")            \n",
    "        \n",
    "    write_pred_texts(cleaned_texts, test_files)\n",
    "    \n",
    "    print(f\"Total cost is {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not used\n",
    "def pipeline1(urls):\n",
    "    \n",
    "    \n",
    "    results=get_all_texts_from_url(urls) \n",
    "    \n",
    "    cleaned_texts, cost=get_only_relevant_text(results, \"gpt-4o-mini\")\n",
    "    \n",
    "    print(f\"Total cost is {cost}\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch the webpage. Status code: 403\n",
      "Failed to fetch the webpage. Status code: 403\n",
      "Failed to fetch the webpage. Status code: 403\n",
      "Failed to fetch the webpage. Status code: 403\n",
      "Failed to fetch the webpage. Status code: 403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▊                                          | 1/25 [00:05<02:19,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.00043259999999999994\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  8%|███▌                                        | 2/25 [00:10<02:04,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.00033389999999999993\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 12%|█████▎                                      | 3/25 [00:19<02:29,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.0006424499999999999\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 16%|███████                                     | 4/25 [00:30<02:59,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.0014332499999999998\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20%|████████▊                                   | 5/25 [00:40<03:03,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.00110685\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 24%|██████████▌                                 | 6/25 [00:46<02:30,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.00022485\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 28%|████████████▎                               | 7/25 [00:54<02:24,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.0006249\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 32%|██████████████                              | 8/25 [01:14<03:21, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.0009755999999999999\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 36%|███████████████▊                            | 9/25 [01:20<02:37,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.00035309999999999996\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 40%|█████████████████▏                         | 10/25 [01:30<02:28,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.00041294999999999993\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 44%|██████████████████▉                        | 11/25 [01:42<02:30, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.00096465\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 48%|████████████████████▋                      | 12/25 [01:48<02:01,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.0003591\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 52%|██████████████████████▎                    | 13/25 [02:00<02:00, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.0004948499999999999\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 56%|████████████████████████                   | 14/25 [02:15<02:06, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.00240675\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60%|█████████████████████████▊                 | 15/25 [02:24<01:47, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.0005748\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 64%|███████████████████████████▌               | 16/25 [02:40<01:52, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.0007321499999999999\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 68%|█████████████████████████████▏             | 17/25 [02:48<01:27, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.0004967999999999999\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 72%|██████████████████████████████▉            | 18/25 [02:49<00:56,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.0002994\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 76%|████████████████████████████████▋          | 19/25 [02:53<00:41,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.00036855\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80%|██████████████████████████████████▍        | 20/25 [02:57<00:29,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.00035444999999999997\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 84%|████████████████████████████████████       | 21/25 [03:07<00:28,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.0008846999999999999\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [03:13<00:20,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.00055185\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [03:19<00:12,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.0005451\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [03:28<00:07,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.00046229999999999996\n",
      "scraped!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [03:32<00:00,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for this scraping:0.00034125\n",
      "scraped!\n",
      "0.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "1.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "2.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "3.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "4.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "5.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "6.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "7.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "8.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "9.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "10.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "11.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "12.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "13.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "14.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "15.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "16.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "17.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "18.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "19.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "20.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "21.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "22.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "23.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "24.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "Total cost is 0.01637715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_files=[str(i)+\".txt\" for i in range(30)]\n",
    "\n",
    "pipeline(urls, test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean cost of scraping is  0.000655086\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean cost of scraping is  {0.01637715/25}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the mean cost of information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_arrestdate0=\"\"\"Your task is to analyze the given context and determine if the starting date of the U.S. ICE (Immigration and Customs Enforcement) operation is mentioned.\n",
    "\n",
    "Key details:\n",
    "\n",
    "The context is a web article about the operation.\n",
    "The starting date must include the day, month, and year to be considered \"mentioned.\"\n",
    "The date might be explicitly stated or inferred through time references (e.g., \"yesterday,\" \"last week\") combined with the article's publication date (if available).\n",
    "\n",
    "Response instructions:\n",
    "\n",
    "Respond True if the starting date is mentioned and all three components (day, month, year) can be identified directly or inferred.\n",
    "Respond False if any component (day, month, year) is missing or cannot be determined.\n",
    "Do not guess or make up information.\n",
    "Context: ### {context} ###\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_arrestdate1=\"\"\"Your task is to analyze the given context and determine the starting date of the U.S. ICE (Immigration and Customs Enforcement) operation.\n",
    "\n",
    "Key details:\n",
    "\n",
    "The context is a web article about the operation.\n",
    "The starting date might be stated explicitly (e.g., a specific date) or implicitly through time references (e.g., \"yesterday,\" \"last week\").\n",
    "Use the article's publication date (if available) to infer the starting date based on time references.\n",
    "Focus only on identifying the operation's starting date. Ignore other dates mentioned in the article.\n",
    "\n",
    "Response format:\n",
    "\n",
    "Only provide the date in the format mm/dd/yy (e.g., 04/15/23).\n",
    "If the starting date cannot be determined, do not guess or infer.\n",
    "Context: ### {context} ###\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_format(response: str)->bool:\n",
    "    \n",
    "    print(f\"RESPONSE IS {response}\")\n",
    "\n",
    "    if response[0] not in \"01\":\n",
    "        return False\n",
    "    if response[1] not in \"0123456789\":\n",
    "        return False\n",
    "    if response[2]!=\"/\":\n",
    "        return False\n",
    "    if response[3] not in \"0123\":\n",
    "        return False\n",
    "    if response[4] not in \"0123456789\":\n",
    "        return False    \n",
    "    if response[5]!=\"/\":\n",
    "        return False\n",
    "    if response[6]!=\"1\":\n",
    "        return False \n",
    "    if response[7] not in \"45678\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "def model1_arrestdate(model_name: str, info: str, info_name: str, prompt0: str, prompt1: str):\n",
    "    \n",
    "    cost=0.0\n",
    "    \n",
    "    PROMPT0 = PromptTemplate(template=prompt0, input_variables=[\"context\"])\n",
    "    \n",
    "    with get_openai_callback() as cb0:\n",
    "        llm = LLMChain(\n",
    "            llm = ChatOpenAI(openai_api_key=api_key,\n",
    "                     temperature=0.01, model=model_name), prompt=PROMPT0)\n",
    "    \n",
    "        response0 = llm.predict(context=info)\n",
    "    \n",
    "        cost +=cb0.total_cost\n",
    "        print(f\"cost for checking is {cb0.total_cost}\")\n",
    "    \n",
    "    if response0!=\"True\" and response0!=\"False\":\n",
    "        print(\"ERROR: Got an unexpected response when looking if the answer is present in the article\")\n",
    "        print(f\"UNEXPECTED RESPONSE: {response0}\")\n",
    "        print(\"The article is:\")\n",
    "        print(info)\n",
    "        print(f\"The article name is {info_name}\")\n",
    "        return None, cost\n",
    "    \n",
    "    if response0==\"False\":\n",
    "        print(\"The information you are asking cannot be found only by looking at the given article\")\n",
    "        print(\"Provide more articles!\")\n",
    "        return None, cost\n",
    "    \n",
    "    PROMPT1 = PromptTemplate(template=prompt1, input_variables=[\"context\"])\n",
    "    \n",
    "    with get_openai_callback() as cb1:\n",
    "        llm = LLMChain(\n",
    "            llm = ChatOpenAI(openai_api_key=api_key,\n",
    "                     temperature=0.01, model=\"gpt-4o\"), prompt=PROMPT1)\n",
    "        \n",
    "        \n",
    "        response1 = llm.predict(context=info)\n",
    "    \n",
    "        cost +=cb1.total_cost\n",
    "        print(f\"cost for extraction is {cb0.total_cost}\")\n",
    "    \n",
    "    if not (check_format(response1)):\n",
    "        print(\"The date is found but is not given in the mm/dd/yy format\")\n",
    "        print(f\"Instead got: {response1}\")\n",
    "        return None, cost\n",
    "    \n",
    "    print(f\"FOUND: {response1}\")\n",
    "    \n",
    "    \n",
    "    return response1, cost\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipline1_arrestdate(directory, files, model_name):\n",
    "    \n",
    "    labels=[]\n",
    "    cost=0.0\n",
    "    \n",
    "    for file in files:\n",
    "        info=\"\"\n",
    "        with open(directory+file,\"r\") as f:\n",
    "            \n",
    "            for line in f:\n",
    "                \n",
    "                info += line\n",
    "                \n",
    "                \n",
    "        response,c= model1_arrestdate(model_name, info, file, prompt_arrestdate0, prompt_arrestdate1)\n",
    "        \n",
    "        cost += c\n",
    "        \n",
    "        labels+=[\"None\" if response==None else response]\n",
    "        \n",
    "        \n",
    "    return labels, cost\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for checking is 0.00271\n",
      "cost for extraction is 0.00271\n",
      "RESPONSE IS 05/18/15\n",
      "FOUND: 05/18/15\n",
      "cost for checking is 0.0024249999999999996\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.003865\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.005125\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.004825\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.001965\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.003845\n",
      "cost for extraction is 0.003845\n",
      "RESPONSE IS 07/18/16\n",
      "FOUND: 07/18/16\n",
      "cost for checking is 0.006175\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.0024649999999999997\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.0029799999999999996\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.00549\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.002195\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.003305\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.0054600000000000004\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.004035\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.004365\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.003335\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.00119\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.002555\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.00194\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.004465\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.0034200000000000003\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.003295\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.003255\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n",
      "cost for checking is 0.0026449999999999998\n",
      "The information you are asking cannot be found only by looking at the given article\n",
      "Provide more articles!\n"
     ]
    }
   ],
   "source": [
    "pred_directory=\"/Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\"\n",
    "pred_files=[str(i)+\".txt\" for i in range(25)]\n",
    "\n",
    "labels, cost= pipline1_arrestdate(pred_directory, pred_files, \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.093985\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean cost of extraction is  0.0037594\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean cost of extraction is  {0.093985/25}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the mean number of articles found for each raid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "andy_directory=\"/Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/search_results_Abnormal_Arrest_Days_All_Days_All_Counties.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(andy_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>search_pattern</th>\n",
       "      <th>StateCountyFIPS</th>\n",
       "      <th>ST</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>FIPSState</th>\n",
       "      <th>FIPSCounty</th>\n",
       "      <th>arrest_date</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date_published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immigration raid Alameda, California</td>\n",
       "      <td>pattern1</td>\n",
       "      <td>6001</td>\n",
       "      <td>CA</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8/6/2015</td>\n",
       "      <td>Readers React: A father deported, kids left be...</td>\n",
       "      <td>https://www.latimes.com/opinion/readersreact/l...</td>\n",
       "      <td>2024-11-16T13:57:00.0000000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immigration raid Alameda, California</td>\n",
       "      <td>pattern1</td>\n",
       "      <td>6001</td>\n",
       "      <td>CA</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8/6/2015</td>\n",
       "      <td>Spurned by local law enforcement, ICE stages e...</td>\n",
       "      <td>https://www.latimes.com/local/politics/la-me-i...</td>\n",
       "      <td>2024-10-27T19:58:00.0000000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Immigration raid Alameda, California</td>\n",
       "      <td>pattern1</td>\n",
       "      <td>6001</td>\n",
       "      <td>CA</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8/6/2015</td>\n",
       "      <td>California gives immigrants here illegally unp...</td>\n",
       "      <td>https://www.latimes.com/local/california/la-me...</td>\n",
       "      <td>2024-11-12T06:30:00.0000000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Immigration raid Alameda, California</td>\n",
       "      <td>pattern1</td>\n",
       "      <td>6001</td>\n",
       "      <td>CA</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8/6/2015</td>\n",
       "      <td>California's Special Restrictions on Who May C...</td>\n",
       "      <td>https://www.littler.com/publication-press/publ...</td>\n",
       "      <td>2024-11-19T23:54:00.0000000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Immigration raid Alameda, California</td>\n",
       "      <td>pattern1</td>\n",
       "      <td>6001</td>\n",
       "      <td>CA</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8/6/2015</td>\n",
       "      <td>UPAC Raids | News, Videos &amp; Articles</td>\n",
       "      <td>https://globalnews.ca/tag/upac-raids/</td>\n",
       "      <td>2024-07-08T20:32:00.0000000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  query search_pattern  StateCountyFIPS  ST  \\\n",
       "0  Immigration raid Alameda, California       pattern1             6001  CA   \n",
       "1  Immigration raid Alameda, California       pattern1             6001  CA   \n",
       "2  Immigration raid Alameda, California       pattern1             6001  CA   \n",
       "3  Immigration raid Alameda, California       pattern1             6001  CA   \n",
       "4  Immigration raid Alameda, California       pattern1             6001  CA   \n",
       "\n",
       "  CountyName  FIPSState  FIPSCounty arrest_date  \\\n",
       "0    Alameda          6           1    8/6/2015   \n",
       "1    Alameda          6           1    8/6/2015   \n",
       "2    Alameda          6           1    8/6/2015   \n",
       "3    Alameda          6           1    8/6/2015   \n",
       "4    Alameda          6           1    8/6/2015   \n",
       "\n",
       "                                               title  \\\n",
       "0  Readers React: A father deported, kids left be...   \n",
       "1  Spurned by local law enforcement, ICE stages e...   \n",
       "2  California gives immigrants here illegally unp...   \n",
       "3  California's Special Restrictions on Who May C...   \n",
       "4               UPAC Raids | News, Videos & Articles   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.latimes.com/opinion/readersreact/l...   \n",
       "1  https://www.latimes.com/local/politics/la-me-i...   \n",
       "2  https://www.latimes.com/local/california/la-me...   \n",
       "3  https://www.littler.com/publication-press/publ...   \n",
       "4              https://globalnews.ca/tag/upac-raids/   \n",
       "\n",
       "                 date_published  \n",
       "0  2024-11-16T13:57:00.0000000Z  \n",
       "1  2024-10-27T19:58:00.0000000Z  \n",
       "2  2024-11-12T06:30:00.0000000Z  \n",
       "3  2024-11-19T23:54:00.0000000Z  \n",
       "4  2024-07-08T20:32:00.0000000Z  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby([\"StateCountyFIPS\",\"CountyName\", \"FIPSState\",\"FIPSCounty\",\"arrest_date\"], as_index=False).agg({\n",
    "    \"url\": lambda x: list(x)  # Combine URLs into a list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StateCountyFIPS</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>FIPSState</th>\n",
       "      <th>FIPSCounty</th>\n",
       "      <th>arrest_date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1081</td>\n",
       "      <td>Lee</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>8/18/2016</td>\n",
       "      <td>[https://www.thenation.com/article/archive/adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1089</td>\n",
       "      <td>Madison</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>11/9/2016</td>\n",
       "      <td>[https://www.justice.gov/opa/pr/former-madison...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1097</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>1/10/2018</td>\n",
       "      <td>[https://www.nbcnews.com/news/us-news/immigrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1097</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>1/13/2017</td>\n",
       "      <td>[https://academic.oup.com/ije/article/46/3/839...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1097</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>1/18/2018</td>\n",
       "      <td>[https://www.courthousenews.com/ice-finalizes-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StateCountyFIPS CountyName  FIPSState  FIPSCounty arrest_date  \\\n",
       "0             1081        Lee          1          81   8/18/2016   \n",
       "1             1089    Madison          1          89   11/9/2016   \n",
       "2             1097     Mobile          1          97   1/10/2018   \n",
       "3             1097     Mobile          1          97   1/13/2017   \n",
       "4             1097     Mobile          1          97   1/18/2018   \n",
       "\n",
       "                                                 url  \n",
       "0  [https://www.thenation.com/article/archive/adv...  \n",
       "1  [https://www.justice.gov/opa/pr/former-madison...  \n",
       "2  [https://www.nbcnews.com/news/us-news/immigrat...  \n",
       "3  [https://academic.oup.com/ije/article/46/3/839...  \n",
       "4  [https://www.courthousenews.com/ice-finalizes-...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4239, 6)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"len_urls\"]=result[\"url\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StateCountyFIPS</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>FIPSState</th>\n",
       "      <th>FIPSCounty</th>\n",
       "      <th>arrest_date</th>\n",
       "      <th>url</th>\n",
       "      <th>len_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1081</td>\n",
       "      <td>Lee</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>8/18/2016</td>\n",
       "      <td>[https://www.thenation.com/article/archive/adv...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1089</td>\n",
       "      <td>Madison</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>11/9/2016</td>\n",
       "      <td>[https://www.justice.gov/opa/pr/former-madison...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1097</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>1/10/2018</td>\n",
       "      <td>[https://www.nbcnews.com/news/us-news/immigrat...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1097</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>1/13/2017</td>\n",
       "      <td>[https://academic.oup.com/ije/article/46/3/839...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1097</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>1/18/2018</td>\n",
       "      <td>[https://www.courthousenews.com/ice-finalizes-...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StateCountyFIPS CountyName  FIPSState  FIPSCounty arrest_date  \\\n",
       "0             1081        Lee          1          81   8/18/2016   \n",
       "1             1089    Madison          1          89   11/9/2016   \n",
       "2             1097     Mobile          1          97   1/10/2018   \n",
       "3             1097     Mobile          1          97   1/13/2017   \n",
       "4             1097     Mobile          1          97   1/18/2018   \n",
       "\n",
       "                                                 url  len_urls  \n",
       "0  [https://www.thenation.com/article/archive/adv...        40  \n",
       "1  [https://www.justice.gov/opa/pr/former-madison...        40  \n",
       "2  [https://www.nbcnews.com/news/us-news/immigrat...        40  \n",
       "3  [https://academic.oup.com/ije/article/46/3/839...        40  \n",
       "4  [https://www.courthousenews.com/ice-finalizes-...        40  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.152158527954704"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"len_urls\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean number of articles found for each raid is  41.152158527954704\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean number of articles found for each raid is  {result['len_urls'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
