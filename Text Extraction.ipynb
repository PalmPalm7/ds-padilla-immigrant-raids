{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Can Erozer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Relevant Text from the Articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"/Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/question_answering_experiement/\"\n",
    "test_files=[\"row28_article_text.txt\",\"row63_article_text.txt\",\"row124_article_text.txt\",\"row126_article_text.txt\",\"row127_article_text.txt\",\"row201_article_text.txt\",\"row202_article_text.txt\",\"row208_article_text.txt\",\"row209_article_text.txt\",\"row267_article_text.txt\",\"row452_article_text.txt\",\"row463_article_text.txt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[\"https://www.cbs58.com/news/34-wisconsin-men-were-arrested-by-immigration-and-customs-enforcement\",\\\n",
    "     \"https://abc7chicago.com/ice-raid-los-angeles-southern-california-four-day-record-244-criminal-immigrants/964411/\",\\\n",
    "     \"https://www.justice.gov/opa/pr/fourteen-alleged-gang-members-and-associates-indicted-charleston-south-carolina-federal\",\\\n",
    "     \"https://www.hjnews.com/education/cache-high-senior-works-stays-positive-despite-dads-deportation/article_96d1bd75-0ac0-5ed5-a885-cad032bee9ea.html\",\\\n",
    "     \"https://www.noozhawk.com/12_arrested_40_in_custody_after_santa_maria_police_led_sweep/\",\\\n",
    "     \"https://www.azcentral.com/story/noticias/2016/06/17/arrestan-39-inmigrantes-durante-redada-en-wisconsin/86054218/\",\\\n",
    "     \"https://www.ice.gov/news/releases/more-100-arrested-los-angeles-area-ice-operation-targeting-convicted-criminal-aliens\",\\\n",
    "     \"https://www.dnainfo.com/chicago/20160811/avondale/this-is-wrong-latino-day-laborer-declares-of-ice-raids-at-job-sites/\",\\\n",
    "     \"https://www.fox8live.com/story/33050506/undocumented-immigrants-arrested-for-operating-new-orleans-sex-brothel/\",\\\n",
    "     \"https://weartv.com/news/local/investigators-man-shared-child-pornography-on-kid-friendly-kik-app\",\\\n",
    "     \"https://www.justice.gov/usao-nh/pr/45-individuals-indicted-participating-fentanyl-trafficking-conspiracy\",\\\n",
    "     \"https://www.nj.com/news/2018/04/ice_arrests_60_in_nj_in_5-day_enforcement_operatio.html\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_extract_info=\"\"\"I have scraped the text of an article, but it contains extra information like side titles, navigation menus, and other irrelevant content. I also have the title of the article. Please extract only the relevant parts of the text that match the main topic of the article based on its title. Ignore anything unrelated to the topic. \n",
    "\n",
    "Here is the contex: {context}\n",
    "\n",
    "Here is the title: {title}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Focus only on the parts of the text that relate directly to the title and the main topic of the article.\n",
    "Don't forget to include the important dates in the article. Also, include the publication date of the article if it is present in the text.\n",
    "Remove any information that is clearly unrelated, like website headers, footers, links, or side titles.\n",
    "Provide the cleaned-up and relevant text as the output.\n",
    "\n",
    "Output format:\n",
    "\n",
    "A cleaned and concise version of the article text, free of irrelevant information. Don't make any explanations.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_similarity_score=\"\"\"I have two texts: the real text of an article and a cleaned version generated by a model. I want to assess how similar these two texts are. Please compare them based on content relevance, key topics, and information retention. Provide a similarity score on a scale of from 0 to 100, where:\n",
    "\n",
    "0 means the texts are completely dissimilar.\n",
    "100 means the texts are identical in terms of information and content.\n",
    "\n",
    "Here is the real text of the article: {text_truth}\n",
    "Here is the cleaned text generated by the model: {text_cleaned}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Compare the two texts based on:\n",
    "Shared key ideas and topics.\n",
    "Retention of the main points in the cleaned text.\n",
    "Any notable missing or extra information in the cleaned text.\n",
    "Provide a similarity score (0â€“100).\n",
    "Don't make any explanations. Just give the score.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this returns a tuple(str, str). The first element of the tuple is the title of the article\n",
    "#and the second element of the tuple is the all of the text present in the url\n",
    "def get_all_texts_from_url(article_urls):\n",
    "    \n",
    "    all_texts=[]\n",
    "    \n",
    "    for url in article_urls:\n",
    "    \n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Get all visible text from the webpage\n",
    "            article_text = soup.get_text(strip=True)\n",
    "            \n",
    "            # Extract text from h1, h2, and p tags\n",
    "            headings_and_paragraphs = []\n",
    "            for tag in soup.find_all(['h1']):\n",
    "                headings_and_paragraphs.append(tag.get_text(strip=True))\n",
    "                \n",
    "            all_texts +=[(headings_and_paragraphs[0], article_text)]\n",
    "                \n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    return all_texts\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=get_all_texts_from_url(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_relevant_text(title_text_pairs, model_name):\n",
    "    \n",
    "    main_text=[]\n",
    "    cost=0\n",
    "    \n",
    "    for title, context in title_text_pairs:\n",
    "    \n",
    "        PROMPT0 = PromptTemplate(template=prompt_extract_info, input_variables=[\"context\", \"title\"])\n",
    "\n",
    "        with get_openai_callback() as cb0:\n",
    "            llm = LLMChain(\n",
    "                llm = ChatOpenAI(openai_api_key=api_key,\n",
    "                         temperature=0.01, model=model_name), prompt=PROMPT0)\n",
    "\n",
    "            response0 = llm.predict(context=context, title=title)\n",
    "\n",
    "            cost +=cb0.total_cost\n",
    "\n",
    "\n",
    "        main_text +=[title + response0]\n",
    "        \n",
    "        \n",
    "    return main_text, cost\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(pred_text, true_text):\n",
    "    \n",
    "    cost=0\n",
    "    \n",
    "    PROMPT0 = PromptTemplate(template=prompt_similarity_score, input_variables=[\"text_truth\", \"text_cleaned\"])\n",
    "\n",
    "    with get_openai_callback() as cb0:\n",
    "        llm = LLMChain(\n",
    "            llm = ChatOpenAI(openai_api_key=api_key,\n",
    "                     temperature=0.01, model=\"gpt-4o-mini\"), prompt=PROMPT0)\n",
    "\n",
    "        response0 = llm.predict(text_cleaned=pred_text, text_truth=true_text)\n",
    "\n",
    "        cost +=cb0.total_cost\n",
    "\n",
    "        \n",
    "    return response0, cost\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pred_texts(pred_texts, out_file_names):\n",
    "    \n",
    "    directory_out=\"/Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\"\n",
    "    \n",
    "    file_names=[]\n",
    "     \n",
    "    for name in out_file_names:\n",
    "        name = \"pred_\" + name\n",
    "        file_names +=[name]\n",
    "        \n",
    "    if len(file_names)!=len(pred_texts):\n",
    "        print(\"pred_text format is not good!\")\n",
    "        return \n",
    "        \n",
    "    for i in range(len(file_names)):\n",
    "\n",
    "        with open(directory_out+file_names[i], \"w\",encoding='utf-8') as file:\n",
    "\n",
    "            file.write(pred_texts[i])\n",
    "            print(f\"{file_names[i]} written to {directory_out}\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_true_texts(test_files):\n",
    "    \n",
    "    true_texts=[]\n",
    "    \n",
    "    for file in test_files:\n",
    "        info=\"\"\n",
    "        with open(directory+file,\"r\") as f:\n",
    "\n",
    "            for line in f:\n",
    "\n",
    "                info += line\n",
    "                \n",
    "        \n",
    "        true_texts += [info]     \n",
    "        \n",
    "        \n",
    "    return true_texts\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(urls, test_files):\n",
    "    \n",
    "    \n",
    "    results=get_all_texts_from_url(urls)\n",
    "    \n",
    "    cleaned_texts, cost=get_only_relevant_text(results, \"gpt-4o-mini\")\n",
    "    \n",
    "    true_texts=load_true_texts(test_files)\n",
    "    \n",
    "    for cleaned_text, true_text in zip(cleaned_texts, true_texts):\n",
    "        \n",
    "        similarity, add_cost=get_similarity(cleaned_text, true_text)\n",
    "        print(f\"Similarity is %{similarity}\")\n",
    "        cost +=add_cost\n",
    "        \n",
    "        \n",
    "    write_pred_texts(cleaned_texts, test_files)\n",
    "    print(f\"Total cost is {cost}\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity is %95\n",
      "Similarity is %Similarity Score: 95\n",
      "Similarity is %85\n",
      "Similarity is %90\n",
      "Similarity is %85\n",
      "Similarity is %100\n",
      "Similarity is %85\n",
      "Similarity is %90\n",
      "Similarity is %Similarity Score: 95\n",
      "Similarity is %85\n",
      "Similarity is %85\n",
      "Similarity is %85\n",
      "pred_row28_article_text.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "pred_row63_article_text.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "pred_row124_article_text.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "pred_row126_article_text.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "pred_row127_article_text.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "pred_row201_article_text.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "pred_row202_article_text.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "pred_row208_article_text.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "pred_row209_article_text.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "pred_row267_article_text.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "pred_row452_article_text.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "pred_row463_article_text.txt written to /Users/canerozer/Desktop/BU/FALL2024/DS701/DS701_Proje/text_extraction_experiment/\n",
      "Total cost is 0.010376399999999997\n"
     ]
    }
   ],
   "source": [
    "pipeline(urls, test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
